{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c337b346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb1baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello world! this is a simple rnn example.\"\n",
    "chars = sorted(list(set(text)))\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for i, c in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e479d361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "print(\"Vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cec7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3453be",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "x_data, y_data = [], []\n",
    "for i in range(len(encoded) - seq_length):\n",
    "    x_data.append(encoded[i:i+seq_length])\n",
    "    y_data.append(encoded[i+seq_length])\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "043c5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=seq_length),\n",
    "    LSTM(256),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c5c377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 64)            1152      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               328704    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 18)                4626      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 334,482\n",
      "Trainable params: 334,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5224e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.8913\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8815\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8710\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8590\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.8445\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8260\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.8017\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.7692\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.7264\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6741\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.6272\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.6420\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.6360\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5937\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5637\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5513\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5446\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5353\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.5202\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4992\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.4732\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4438\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4117\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.3770\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3383\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2942\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2467\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.1935\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.1297\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0716\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.0190\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9631\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9090\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8456\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.7866\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.7145\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6493\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.5922\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.5342\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4830\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4300\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3841\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3450\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2966\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2406\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.1928\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.1575\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.1025\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0382\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e29916050>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_data, y_data, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6ef3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, length=100):\n",
    "    # If seed text is shorter, pad it (left side) with the first character of the dataset\n",
    "    if len(seed_text) < seq_length:\n",
    "        seed_text = seed_text.rjust(seq_length, text[0])\n",
    "    # If seed text is longer, trim it\n",
    "    elif len(seed_text) > seq_length:\n",
    "        seed_text = seed_text[-seq_length:]\n",
    "\n",
    "    input_seq = [char2idx[c] for c in seed_text]\n",
    "\n",
    "    for _ in range(length):\n",
    "        x = np.array([input_seq[-seq_length:]])  # always 10 chars\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = np.random.choice(len(preds), p=preds)\n",
    "        input_seq.append(next_index)\n",
    "\n",
    "    return ''.join(idx2char[i] for i in input_seq)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cda82dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "\n",
      "ello world! htstsiia  ispamllern  ..h srlee s s    ple empeeehor.a.ooi!si sthsipi    saplnerrn  rd enilpn a e.\n",
      "nn example..ppxwtw o! hx.s is  epea lrep.wlri   ms s eleeer l .ladre     siilerlele n ips .r.een  se exx.neer.\n",
      "hhhhhhhhhi   isssiplll  i   tsi llele i  diixm.lelmaresxd aimn e e lle..nelp i .neeaiellpe...end tp.  leninier\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerated text:\\n\")\n",
    "print(generate_text(\"hello world\", length=100))   \n",
    "print(generate_text(\"rnn example\", length=100)) \n",
    "print(generate_text(\"hi\", length=100))          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you are all resolved rather to die than to famish?\n",
      "\n",
      "all:\n",
      "resolved. resolved.\n",
      "\n",
      "first citizen:\n",
      "first, you know caius marcius is chief enemy to the people.\n",
      "\n",
      "all:\n",
      "we know't, we know't.\n",
      "\n",
      "first citizen:\n",
      "let us kill him, and we'll have corn at our own price.\n",
      "is't a verdict?\n",
      "\n",
      "all:\n",
      "no more talking on't; let it be done: away, away!\n",
      "\n",
      "second citizen:\n",
      "one word, good citizens.\n",
      "\n",
      "first citizen:\n",
      "we are accounted poor citizens, the patricians good.\n",
      "what authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know i\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.utils import get_file\n",
    "# path = get_file(\"shakespeare.txt\",\n",
    "#                 \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "# text = open(path, 'r').read().lower()\n",
    "# print(text[:1000])  # Print first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c238bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "121dc7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file(\"shakespeare.txt\",\n",
    "                \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
    "text = open(path, 'r').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2807bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = np.array(chars)\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39648e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac9d1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5666a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cdbf12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (dataset.shuffle(BUFFER_SIZE)\n",
    "           .batch(BATCH_SIZE, drop_remainder=True))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8e4514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[BATCH_SIZE, None]),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                              return_sequences=True,\n",
    "                              stateful=True,\n",
    "                              recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "359a5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a7680fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 21s 116ms/step - loss: 2.5115\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 22s 122ms/step - loss: 1.9961\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 19s 109ms/step - loss: 1.7804\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 20s 114ms/step - loss: 1.6473\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 21s 118ms/step - loss: 1.5612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "EPOCHS = 5\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units,\n",
    "                              return_sequences=True,\n",
    "                              stateful=True,\n",
    "                              recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "model2.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522678d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=500):\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047eedae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 19s 100ms/step - loss: 2.6331\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 2.0982\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 18s 103ms/step - loss: 1.8741\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 1.7177\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 19s 105ms/step - loss: 1.6168\n",
      "hello: think you wefe and true: now them sheel'd discokn me a moss lord.\n",
      "\n",
      "nurst:\n",
      "ain; had young vistority.\n",
      "\n",
      "jawnich:\n",
      "i crund!\n",
      "\n",
      "hastings:\n",
      "if i have let my lords, and will demp wornd loblester\n",
      "and were brout or, bust me no love achingers?\n",
      "\n",
      "caunfoly verimation or make a sight these though romember:\n",
      "i would hetcharie and desi:\n",
      "livit not, it dough off with glow:\n",
      "sor, how, lady frownill in youths.\n",
      "\n",
      "first sentariag afording a.\n",
      "this is to: sir, cannock,\n",
      "he him like what jest thos in his city as youble some tha\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model2, start_string=\"hello: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdef60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
